---
date: Tue Nov 26 2024
title: 'Meta Llama Hackathon: Toronto'
tagline: I got to use a H100 GPU!
preview: >-
  "Welcome to Meta's first Llama hackathon in Toronto!! We're bringing together
  150 of Canada's best AI developers to build using Llama." I read this and
  never thought that I'd be a part of this hackathon.
image: /images/IMG_2010.JPG
type: ''
---
# Building the Future with Llama

Last weekend, I had the incredible opportunity to attend Meta’s first-ever Llama Hackathon in Toronto, a gathering of 150 of Canada’s brightest AI minds. The event brought together developers, creators, and innovators to explore the limitless possibilities of Meta’s Llama models, and it turned out to be one of the most inspiring weekends of my life.  

### The People: Specialists in AI and Beyond  
One of the most memorable moments was meeting Michael Chiang, the co-founder of **Ollama**. Not only did I get some awesome stickers and a water bottle (which I’ve already added to my collection), but I also got to hear about his inspiring journey—from studying at the University of Waterloo to launching a startup through Y Combinator, and eventually returning to Toronto to build something truly impactful. His story showed me what’s possible with dedication and a clear vision, and it resonated deeply with my own ambitions in tech.  

What truly stood out to me at the hackathon was the sheer diversity of talent. Everyone I met was incredibly specialized in their field, whether it was multimodal models, reinforcement learning, or human-computer interaction. Conversations weren’t just technical—they were passionate. Each person I spoke to shared their unique perspective on AI, and their enthusiasm was infectious. It was impossible not to feel energized about the future of artificial intelligence.  

Being surrounded by such innovative thinkers reignited my passion for AI. It reminded me why I love this field—its ability to solve real-world problems and transform lives. The hackathon didn’t just challenge me to build; it inspired me to think bigger, explore deeper, and dream bolder.  

### Introducing **See**: Redefining Accessibility  
The hackathon wasn’t just a networking event—it was a chance to create. My team and I built **See**, an AI-powered assistive tool designed to empower visually impaired individuals. Our goal was to move beyond the limitations of traditional assistive technologies, which often rely on complex interfaces, lengthy prompts, or external volunteers for assistance. With **See**, we wanted to provide independence in the most intuitive way possible.  

#### How **See** Works  
**See** uses Meta’s Llama models (Llama 70B, Llama 90B Multimodal, and Llama 3.2), optimized on Nebius AI infrastructure powered by NVIDIA H100 Tensor Core GPUs, to deliver real-time, context-aware assistance. The tool is designed with accessibility at its core, featuring:  

1. **Voice-First Interaction**: Users can ask questions like “What’s around me?” or “Read this menu” through natural voice commands, eliminating the need for typing.  
2. **Context-Aware Responses**: The AI adapts to the user’s environment and preferences, providing accurate and relevant information instantly.  
3. **Personalized Experience**: Users can customize the AI’s speech style and persona, making it feel like a true companion.  
4. **Cross-Device Compatibility**: **See** works seamlessly on both computers and mobile phones, ensuring accessibility anytime, anywhere.  
5. **Offline Functionality**: The system operates without an internet connection, addressing a critical need for reliability in all settings.  

#### Live Use Cases of **See**  
- **Navigation Assistance**: A single tap lets users know what’s around them, such as “There’s a coffee shop entrance 10 feet ahead, slightly to your right.”  
- **Text Reading**: **See** can read printed or digital text aloud, such as a restaurant menu or sign.  
- **Environmental Awareness**: Users can receive detailed descriptions of their surroundings to navigate independently and confidently.  

#### Why It Matters  
Assistive technologies have historically been complex, unintuitive, and frustrating for blind users. **See** reimagines this experience with an intuitive, user-centric design powered by cutting-edge AI. It’s not just a tool—it’s a pathway to independence, helping visually impaired individuals interact with the world on their terms.  

### Final Thoughts  
This hackathon wasn’t just about the technology—it was about the people, the stories, and the collective drive to create something meaningful. Meeting industry leaders like Michael Chiang, collaborating with talented peers, and pushing the boundaries of AI has left me more excited than ever about what’s next.  

Here’s to many more weekends of building, learning, and dreaming big!
